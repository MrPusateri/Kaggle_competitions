{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "This notebook use a kaggle open data-set in order to develop an experiment in order to manage unbalanced data for a classification problem. The name of the open-data set is [Health insurance cross sell prediction](https://www.kaggle.com/datasets/anmolkumar/health-insurance-cross-sell-prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utilities import ut_standard_col_name, ut_distinct_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 381109 entries, 0 to 381108\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   id                    381109 non-null  int64  \n",
      " 1   gender                381109 non-null  object \n",
      " 2   age                   381109 non-null  int64  \n",
      " 3   driving_license       381109 non-null  int64  \n",
      " 4   region_code           381109 non-null  float64\n",
      " 5   previously_insured    381109 non-null  int64  \n",
      " 6   vehicle_age           381109 non-null  object \n",
      " 7   vehicle_damage        381109 non-null  object \n",
      " 8   annual_premium        381109 non-null  float64\n",
      " 9   policy_sales_channel  381109 non-null  float64\n",
      " 10  vintage               381109 non-null  int64  \n",
      " 11  response              381109 non-null  int64  \n",
      "dtypes: float64(3), int64(6), object(3)\n",
      "memory usage: 34.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# import input (training) data\n",
    "input_df = pd.read_csv(\"/Users/lorenzopusateri/Documents/01_studio/04_kaggle/Kaggle/competitions/insurance_cross_selling/data/train.csv\")\n",
    "# standardize column names\n",
    "input_df = ut_standard_col_name(input_df)\n",
    "input_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables meaning\n",
    "\n",
    "One of the first operation with the data is like when we meet a new person in the real life. We have to exchange some basic information. In that case we have already some information the challenge with the metadata, however, is good prectice start with numerical information.\n",
    "\n",
    "Columns are:\n",
    "* `id`, Unique ID for the customer.\n",
    "* `gender`, Gender of the customer.\n",
    "* `age`, Age of the customer.\n",
    "* `driving_license`, 0: Customer does not have DL, 1: Customer already has DL\n",
    "* `region_code`, unique code for the region of the customer.\n",
    "* `previously_insured`, 1: Customer already has Vehicle Insurance, 0: Customer doesn't have Vehicle Insurance.\n",
    "* `vehicle_age`, age of the vehicle.\n",
    "* `vehicle_damage`, 1: Customer got his/her vehicle damaged in the past. 0: Customer didn't get his/her vehicle damaged in the past.\n",
    "* `annual_premium`, The amount customer needs to pay as premium in the year.\n",
    "* `policy_sales_channel`, Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n",
    "* `vintage`, Number of Days, Customer has been associated with the company.\n",
    "* `response`, 1: Customer is interested, 0: Customer is not interested.\n",
    "\n",
    "Since I am italian I want to simulate the problem like the real scenario in Italy, where is not permitted to use the gender information in insurance companies for pricing and other reseach. Hence, I will drop that information. Moreover, I drop also the `id` since it could be useful only if we have other data to merge where there it is used as key. We do not have other data to merge and for prediction the `id` variable is useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = input_df.copy()\n",
    "train_df = train_df.drop(columns=['gender', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target variable\n",
    "\n",
    "Now it is time to inspect the target variable in the training data-set. The problem we are facing is a classification problem. Thanks to the traing data we need to build a model that is able to classify new records with the same kind of information (variables) in the correct way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of records with response=0: 88.0%\n",
      "Percentage of records with response=1: 12.0%\n"
     ]
    }
   ],
   "source": [
    "value_counts_response = train_df.response.value_counts().to_dict()\n",
    "\n",
    "# Compute percentage of target classes\n",
    "perc_label_0 = value_counts_response[0]/len(train_df)\n",
    "perc_label_1 = value_counts_response[1]/len(train_df)\n",
    "print(f\"Percentage of records with response=0: {round(perc_label_0, 2)*100}%\")\n",
    "print(f\"Percentage of records with response=1: {round(perc_label_1, 2)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced data-set\n",
    "\n",
    "As we can see there are much more records with `response=0`. That scenario in Machine Learning (aka ML) jargon is called _imbalanced data_. In the book _Introduction to Machine Learning with Python_, written by Andreas C. Müller and Sarah Guido they talk about the problem of imbalanced datasets.\n",
    "\n",
    "«Types of errors play an important role when one of two classes is much more frequent than the other one.\n",
    "\n",
    "This is very common in practice; a good example is click-through prediction, where each data point represents an “impression,” an item that was shown to a user. This item might be an ad, or a related story, or a related person to follow on a social media site. \n",
    "\n",
    "The goal is to predict whether, if shown a particular item, a user will click on it (indicating they are interested). Most things users are shown on the Internet (in particular, ads) will not result in a click. You might need to show a user 100 ads or articles before they find something interesting enough to click on.\n",
    "\n",
    "This results in a dataset where for each 99 “no click” data points, there is 1 “clicked” data point; in other words, 99% of the samples belong to the “no click” class. Datasets in which one class is much more frequent than the other are often called imbalanced datasets, or datasets with imbalanced classes.\n",
    "\n",
    "In reality, imbalanced data is the norm, and it is rare that the events of interest have equal or even similar frequency in the data.»\n",
    "\n",
    "Hence we have to use some algorithms to manage that kind of errors and create a balanced data-set. There are differnt methods to deal with imbalanced datasets. Those methods can be splitted into two main macro categories:\n",
    "1. Under-sampling, we **reduce** the dimension of the imbalanced dataset removing records that has the label that appears most of the time. That famility of methods is useful when we have a huge amount of data and we can lose some information.\n",
    "2. Over-sampling, we **increase** the dimension of the imbalanced dataset adding new records. That family of methods is useful since we do not lose original information. However, depending on the algorithm we decide to use we can add more or less noise and/or errros in the data.\n",
    "\n",
    "The method I decide to use in this notebook is the _Synthetic Minority Over-sampling Technique for Nominal and Continuous_ (aka SMOTENC) implemented in the Python library `imbalanced-learn`. I decided to use it since I have both categorical (Nominal) and continuous variables. To further discussion on the over-sampling method that are implemented in the Python library `imbalanced-learn` click on the following [link](https://imbalanced-learn.org/stable/over_sampling.html#smote-adasyn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
